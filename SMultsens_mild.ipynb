{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48588bc0-c255-49d2-a039-1a2845b201d7",
   "metadata": {},
   "source": [
    "<img src=\"img/ibidat.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e798e-ca3d-40da-ae5a-fd75dc9c093c",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno vamos a hacer un estudio de simulación aplicando cada algoritmo que se ha reseñado en el TFM a un dataset simulado con dos atributos sensibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab952a-8d5e-4540-b2e6-0da1fe09c666",
   "metadata": {},
   "source": [
    "# Carga de librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be38405-c18a-401d-8e81-8ad1e39a25b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Con esta celda, podemos hacer cambios en cualquier script/libreria y verlo reflejado en las funciones del notebook.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473c1a0-a0fe-459b-b556-cc76d9815039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CARGAMOS LIBRERIAS\n",
    "# Obs: La primera vez puede tardar en cargar porque AIF360 pre-compila algunas cosas de tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargamos librerías de sklearn\n",
    "# vanilla LR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Cargamos xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Cargamos algoritmos de fairness\n",
    "# Pre-procesado\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "# In-procesado\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "# Post-procesado\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification # when scores are given\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing # when scores (R) are given\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing # when labels (Y^hat) are given\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "#Simulacion\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# TF para adversarial debiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Librería local con funciones de ayuda extra\n",
    "import utils\n",
    "\n",
    "seed = 12345 # fijamos la semilla de números aleatorios para reproducir exactamante el notebook\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a704d6-8a1e-4bed-b82c-de0e86af71fc",
   "metadata": {},
   "source": [
    "# Selección de dataset.\n",
    "\n",
    "El siguiente bloque permite seleccionar el conjunto de datos sobre el que trabajar. Debe recordarse que el conjunto de `homecredit` es mucho mayor por lo que tardará más tiempo en ejecutarse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b1324-25a2-49b0-8109-6f0984042cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def inv_logit(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def simulated_dataset(n_obs, n_var, n_sens, mean_vec, cov, beta, gamma):\n",
    "    \n",
    "    p = n_var + n_sens\n",
    "    coef = np.concatenate((gamma, beta))\n",
    "        \n",
    "    X = np.random.multivariate_normal(mean_vec, cov, size = n_obs)\n",
    "    probs_var = inv_logit(X)\n",
    "    bin = np.random.binomial(1, probs_var)\n",
    "    \n",
    "    probs_Y = inv_logit(coef.dot(X.T))\n",
    "    Y = np.random.binomial(1, probs_Y)\n",
    "    \n",
    "    names = ['0' for i in range(p+1)]\n",
    "    sens = ['0' for i in range(n_sens)]\n",
    "    var = 1\n",
    "    \n",
    "    names[0] = 'response'\n",
    "    \n",
    "    for i in range(1,p+1):\n",
    "        if i-1 < n_sens:\n",
    "            sens[i-1] = 'sens ' + str(i)\n",
    "            names[i] = 'sens ' + str(i)\n",
    "        if i-1 >= n_sens:\n",
    "            names[i] = 'var ' + str(var)\n",
    "            var += 1\n",
    "    \n",
    "    names = np.array(names)\n",
    "    sens = np.array(sens)\n",
    "    response = np.array(['response'])\n",
    "    \n",
    "    Y = Y.reshape((n_obs,1))\n",
    "    \n",
    "    bin = np.concatenate((Y, bin), axis = 1)\n",
    "    \n",
    "    data_pd = pd.DataFrame(bin, columns = names)\n",
    "    data = BinaryLabelDataset(df = data_pd, favorable_label = 1, unfavorable_label = 0, label_names = response, protected_attribute_names = sens)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb01964-e49f-46f0-856b-de3082b4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "n_var = 30\n",
    "n_sens = 2\n",
    "mean_vec = np.zeros(n_var+n_sens)\n",
    "cov = np.identity(n_var+n_sens)\n",
    "beta = np.ones(n_var)\n",
    "gamma = np.ones(n_sens)\n",
    "\n",
    "dataset1 = simulated_dataset(n_obs, n_var, n_sens, mean_vec, cov, beta, gamma)\n",
    "\n",
    "#dataset.favorable_label = 1\n",
    "#dataset.unfavorable_label = 0\n",
    "\n",
    "new_data = dataset1.convert_to_dataframe()[0]\n",
    "\n",
    "sens1 = new_data.loc[:,'sens 1']\n",
    "sens2 = new_data.loc[:,'sens 2']\n",
    "new_sens = sens1.copy()\n",
    "new_sens[sens2 == 1] = 1\n",
    "\n",
    "new_data = new_data.drop(['sens 1', 'sens 2'], axis = 1)\n",
    "new_data['sens 1'] = new_sens\n",
    "\n",
    "names = np.array(new_data.columns)\n",
    "sens = np.array(['sens 1'])\n",
    "response = np.array(['response'])\n",
    "\n",
    "dataset = BinaryLabelDataset(df = new_data,\n",
    "                             favorable_label = 1,\n",
    "                             unfavorable_label = 0,\n",
    "                             label_names = response,\n",
    "                             protected_attribute_names = sens)\n",
    "\n",
    "# Separamos el conjunto de datos en train, validate, test\n",
    "data_train, vt = dataset.split([0.7], shuffle=True, seed=seed)\n",
    "data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "data_train2, vt2 = dataset1.split([0.7], shuffle=True, seed=seed)\n",
    "data_val2, data_test2 = vt2.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "# Obtenemos los indicadores del grupo sensible\n",
    "sensitive_attribute = dataset.protected_attribute_names\n",
    "privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset)\n",
    "    \n",
    "print(f'Dimensiones del conjunto de datos: {dataset.features.shape}')\n",
    "print(\"Grupos privilegiados:\", privileged_groups)\n",
    "print(\"Grupos no privilegiados:\", unprivileged_groups)\n",
    "print(\"Label del grupo favorable:\", dataset.favorable_label)\n",
    "print(\"Label del grupo desfavorable:\", dataset.unfavorable_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc846922-b51e-42f7-ba70-f80176d8c20e",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "Para esta sección, creamos un diccionario de métricas y modelos para poder acceder los resultados de todos los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61957d07-fe06-40c8-9176-65bb30e28891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inicializamos diccionarios\n",
    "models = dict()\n",
    "\n",
    "# Rango de umbrales para evaluar el score de los modelos\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3e46d-a36f-4124-aa39-5b33e2b11e3d",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c59d73-fc36-4e01-97ea-90d58ca0bfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'logistic_regression'\n",
    "fairness_method = ''\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'logisticregression__sample_weight': data_train_copy.instance_weights}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = make_pipeline(\n",
    "            #StandardScaler(),\n",
    "            LogisticRegression(solver='liblinear', random_state=seed))\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d01f4-0db7-456e-ad01-c7b9b304bfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy, \n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82f0f9-6553-4d42-b659-e20ebddee999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Test set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_test[model_name+fairness_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539925b5-4481-46e0-832e-d41283b676c8",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ec750-56cf-46b2-a45b-2f7716a3b040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'xgb_classifier'\n",
    "fairness_method = ''\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = XGBClassifier(**fit_params)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c37c70-85a5-4330-b914-65af16874160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28313bf-3f5f-4d69-aa4f-4e6c3d12cdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Test set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_test[model_name+fairness_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721603e-0d73-4acd-972f-f76227b22e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Técnicas de preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713b654-db5a-485f-a78f-ed5b7f26e663",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reponderación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abe48e-485d-49b8-a384-032453f9a579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_reweighting'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos el pre-procesado al conjunto train\n",
    "# Inicializamos la clase de pre-procesado \n",
    "PreProcessor = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "# Pre-procesamos (modifica los datos)\n",
    "PreProcessor.fit(data_train_copy)\n",
    "\n",
    "data_train_copy = PreProcessor.transform(data_train_copy)\n",
    "data_val_copy = PreProcessor.transform(data_val_copy)\n",
    "data_test_copy = PreProcessor.transform(data_test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218cc5fc-cfc0-4bc1-9cfc-b93eed8b0215",
   "metadata": {},
   "source": [
    "De aquí en adelante, se aplica el modelo de preferencia de manera estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301d39c-1a7d-4119-b850-9b5dd8424826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Regresión logística\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'logisticregression__sample_weight': data_train_copy.instance_weights}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = make_pipeline(\n",
    "            #StandardScaler(),\n",
    "            LogisticRegression(solver='liblinear', random_state=seed))\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel(), **fit_params)\n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4661699-21d8-44cc-9dd0-718d3f87f8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'xgb_classifier'\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = XGBClassifier(**fit_params)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel())\n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac1ebc-5d89-4e3d-897e-cd44e7e7371f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Eliminador de impacto dispar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a243f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_di_remover'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Parámetros del modelo\n",
    "nivel_reparacion = 0.5 # un valor 0 inidica ausencia de reparación (datos se mantienen igual) \n",
    "                       # un valor 1 indica reparación completa (se puede perder nivel predictivo).\n",
    "                       # Se puede establecer cualquier valor entre 0 y 1.\n",
    "\n",
    "# Hacemos el pre-procesado al conjunto train\n",
    "# Inicializamos la clase de pre-procesado \n",
    "PreProcessor = DisparateImpactRemover(\n",
    "    repair_level=nivel_reparacion,\n",
    "    sensitive_attribute=sensitive_attribute\n",
    ")\n",
    "# Pre-procesamos (modifica los datos)\n",
    "PreProcessor.fit(data_train_copy)\n",
    "\n",
    "data_train_copy = PreProcessor.fit_transform(data_train_copy)\n",
    "data_val_copy = PreProcessor.fit_transform(data_val_copy)\n",
    "data_test_copy = PreProcessor.fit_transform(data_test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1136b",
   "metadata": {},
   "source": [
    "Aplicamos los métodos de regresión logística y XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a7298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Regresión logística\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'logisticregression__sample_weight': data_train_copy.instance_weights}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = make_pipeline(\n",
    "            #StandardScaler(),\n",
    "            LogisticRegression(solver='liblinear', random_state=seed))\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel(), **fit_params)\n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22155db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'xgb_classifier'\n",
    "\n",
    "# Parámetros del modelo\n",
    "fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "# Asignamos el modelo al diccionario\n",
    "models[model_name+fairness_method] = XGBClassifier(**fit_params)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy.features, data_train_copy.labels.ravel())\n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c4d10-fc48-4210-8f95-9253bfaab37d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Técnicas de inprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817df1d7-539c-4cca-ad6b-bf55dce77e60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Eliminador de prejuicios por regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167fbe7-176e-4e7a-9863-91fd4d16eb20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "model_name = 'prejudice_remover'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Parámetros del modelo\n",
    "η = 50.0 # hyper-parameter de regularización\n",
    "\n",
    "# Inicializamos el modelo y lo asignamos al diccionario\n",
    "models[model_name] = PrejudiceRemover(sensitive_attr=sensitive_attribute, eta=η)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "models[model_name] = models[model_name].fit(data_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f04f6-9903-41c5-9a27-e6c7d94e2781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name] = utils.describe_metrics(metrics_sweep[model_name])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name], \n",
    "    threshold=metrics_best_thresh_validate[model_name]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name])\n",
    "a.set_title(model_name)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde49761-1f41-4c5c-8655-91448a6d7825",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Algoritmo de metafairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c77a28-9902-4a3e-bec1-cc271a22b416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "model_name = 'meta_fairness'\n",
    "\n",
    "# Parámetros del modelo\n",
    "# hyper-parameter de regularización\n",
    "τ = 0.8\n",
    "# métricas fairness a optimizar\n",
    "quality_constraints = ['sr', 'fdr'] # sr: statistical rate, fdr: false discovery rate\n",
    "\n",
    "# Optimizamos un modelo nuevo por cada métrica\n",
    "for quality in quality_constraints:\n",
    "    \n",
    "    # Hacemos una copia de los datasets\n",
    "    data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "    \n",
    "    # nombramos al modelo con su métrica específica\n",
    "    model_name_quality = '{}_{}'.format(model_name, quality)\n",
    "    \n",
    "    # Inicializamos el modelo y lo asignamos al diccionario\n",
    "    models[model_name_quality] = MetaFairClassifier(tau=τ, sensitive_attr=sensitive_attribute, type=quality, seed=seed)\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    models[model_name_quality] = models[ model_name_quality ].fit( data_train_copy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed9fc9-6d6a-421c-9047-13d46ab73872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "for quality in quality_constraints:\n",
    "    \n",
    "    name_suffix = '_{}'.format(quality)\n",
    "    print(f'Results for {model_name+name_suffix}')\n",
    "\n",
    "    # Evaluamos al modelo para un rango de umbrales\n",
    "    metrics_sweep[model_name+name_suffix] = utils.metrics_threshold_sweep_mult(\n",
    "        dataset=data_val_copy,\n",
    "        dataset2=data_val2_copy,\n",
    "        model=models[model_name+name_suffix],\n",
    "        thresh_arr=thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluamos las métricas para el mejor umbral\n",
    "    metrics_best_thresh_validate[model_name+name_suffix] = utils.describe_metrics(metrics_sweep[model_name+name_suffix])\n",
    "\n",
    "    # Usando el mejor umbral, calculamos las métricas en el test set\n",
    "    metrics_best_thresh_test[model_name+name_suffix] = utils.compute_metrics_mult(\n",
    "        dataset=data_test_copy, \n",
    "        dataset2=data_test2_copy,\n",
    "        model=models[model_name+name_suffix], \n",
    "        threshold=metrics_best_thresh_validate[model_name+name_suffix]['best_threshold'])\n",
    "\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name+name_suffix])\n",
    "    print('\\n')\n",
    "\n",
    "    # Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "    # utils.plot_fairness_and_accuracy(metrics_sweep[model_name+name_suffix])\n",
    "    # plt.title(model_name+name_suffix)\n",
    "    \n",
    "    f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+name_suffix])\n",
    "    a.set_title(model_name+name_suffix)\n",
    "    f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17136a20-1db8-40f2-9258-dca562b07c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Eliminación de prejuicios adversarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608328b-f143-4b64-9eec-bf77c02fe9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'NN_adversarial_debiasing'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "models[model_name] = AdversarialDebiasing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    scope_name = 'debiased_classifier',\n",
    "    debias=True,\n",
    "    sess=sess,\n",
    "    num_epochs=80)\n",
    "\n",
    "models[model_name].fit(data_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c3d01-00f3-444b-ab04-b64ba37b396a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name] = utils.describe_metrics(metrics_sweep[model_name])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name], \n",
    "    threshold=metrics_best_thresh_validate[model_name]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name])\n",
    "a.set_title(model_name)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc8a18-6bc9-40e3-84a3-849ff51bfa6e",
   "metadata": {},
   "source": [
    "#### Eliminación de prejuicios adversarios: Red neuronal sin eliminación de prejuicios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ada185-6303-497f-8530-6e928e6129ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Nombramos al modelo\n",
    "model_name = 'Basic_NN_no_adversarial_debiasing'\n",
    "\n",
    "# Hacemos una copia de los datasets\"\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "models[model_name] = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                          unprivileged_groups = unprivileged_groups,\n",
    "                                          scope_name = 'biased_classifier',\n",
    "                                          debias=False,\n",
    "                                          sess=sess,\n",
    "                                          num_epochs=125)\n",
    "models[model_name].fit(data_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb31736-e353-440e-8621-08c3d3f990e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name] = utils.metrics_threshold_sweep_mult(\n",
    "    dataset=data_val_copy,\n",
    "    dataset2=data_val2_copy,\n",
    "    model=models[model_name],\n",
    "    thresh_arr=thresh_sweep\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name] = utils.describe_metrics(metrics_sweep[model_name])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name] = utils.compute_metrics_mult(\n",
    "    dataset=data_test_copy, \n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name], \n",
    "    threshold=metrics_best_thresh_validate[model_name]['best_threshold'])\n",
    "\n",
    "display(Markdown(\"**Validation set results**\"))\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name])\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name])\n",
    "a.set_title(model_name)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53fa71-a143-41d5-a588-b397de752484",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Técnicas de postprocesado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30be21-dc70-465d-a828-caf87bf47886",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clasificación de rechazo de opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e229f4c-a0ce-4cc6-88d4-1f0319c804ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "fairness_method = '_reject_option'\n",
    "\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# métricas fair a optimizar\n",
    "fair_metrics = {'spd': \"Statistical parity difference\", 'aod': \"Average odds difference\", 'eod': \"Equal opportunity difference\"}\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "# Optimizamos un modelo nuevo por cada métrica\n",
    "for key_metric in fair_metrics:\n",
    "    \n",
    "    # nombramos al modelo con su métrica específica\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    models[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics[key_metric],\n",
    "        metric_lb=-0.01, metric_ub=0.01)\n",
    "\n",
    "    # Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "    models[model_name_metric] = models[model_name_metric].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dac01-64d3-469d-a72d-3fd83cd09962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Validation set results for logistic regression**\"))\n",
    "for key_metric in fair_metrics:\n",
    "    \n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_val_copy, \n",
    "        dataset_preds=data_val_preds, \n",
    "        dataset2 = data_val2_copy,\n",
    "        model=models[model_name_metric], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    print(model_name_metric)\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name_metric])\n",
    "    print('\\n')\n",
    "    \n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_test_copy, \n",
    "        dataset2 = data_test2_copy,\n",
    "        dataset_preds=data_test_preds, \n",
    "        model=models[model_name_metric], \n",
    "        required_threshold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a778bb-a5dc-40d8-9362-b56410cc461d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Nombramos al modelo\n",
    "fairness_method = '_reject_option'\n",
    "\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'xgb_classifier'\n",
    "\n",
    "# métricas fair a optimizar\n",
    "fair_metrics = {'spd': \"Statistical parity difference\", 'aod': \"Average odds difference\", 'eod': \"Equal opportunity difference\"}\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "data_train2_preds = utils.update_dataset_from_model2(data_train_copy, data_train2_copy, models[model_name])\n",
    "data_val2_preds = utils.update_dataset_from_model2(data_val_copy, data_val2_copy, models[model_name])\n",
    "data_test2_preds = utils.update_dataset_from_model2(data_test_copy, data_test2_copy, models[model_name])\n",
    "\n",
    "# Optimizamos un modelo nuevo por cada métrica\n",
    "for key_metric in fair_metrics:\n",
    "    \n",
    "    # nombramos al modelo con su métrica específica\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    models[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics[key_metric],\n",
    "        metric_lb=-0.01, metric_ub=0.01)\n",
    "\n",
    "    # Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "    models[model_name_metric] = models[model_name_metric].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd1721-8104-4587-ae4b-666826fa4356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Validation set results for xgb classifier**\"))\n",
    "for key_metric in fair_metrics:\n",
    "    \n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_val_copy, \n",
    "        dataset_preds=data_val_preds, \n",
    "        dataset2 = data_val2_copy,\n",
    "        model=models[model_name_metric], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    print(model_name_metric)\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name_metric])\n",
    "    print('\\n')\n",
    "    \n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_test_copy, \n",
    "        dataset_preds=data_test_preds, \n",
    "        dataset2 = data_test2_copy,\n",
    "        model=models[model_name_metric], \n",
    "        required_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3055c6c-a395-4550-b22a-5491fbea99fa",
   "metadata": {},
   "source": [
    "### Escalado de Platt por grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41538c5d-3d48-4813-a30f-94fdb9347129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fairness_method = '_platt_scaling'\n",
    "model_names = ['logistic_regression', 'xgb_classifier']\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy = True), data_val.copy(deepcopy = True), data_test.copy(deepcopy = True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Hacemos una copia de las predicciones del modelo de referencia\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    data_val_preds = utils.update_dataset_from_model(data_val, models[model_name], class_thresh = model_thresh)\n",
    "    data_val_preds_m = utils.update_dataset_from_model2(data_val, data_val2_copy, models[model_name], class_thresh = model_thresh)\n",
    "    ## Plat Scaling:\n",
    "    #1. dividir el validation data usando los groups sensibles\n",
    "    data_val_preds_priv, data_val_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = data_val_preds, privileged_group_label = list((privileged_groups[0].values()))[0])\n",
    "    #2. copia el validation data para guardar los scores\n",
    "    data_val_preds2_m = data_val_preds_m.copy(deepcopy = True)\n",
    "    data_val_preds2 = data_val_preds.copy(deepcopy = True)\n",
    "    #3. hacer un modelo para cada grupo\n",
    "    sensitive_groups_data = {'priv': [data_val_preds_priv, priv_indices],\n",
    "                             'unpriv': [data_val_preds_unpriv, unpriv_indices]}\n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data.items():\n",
    "\n",
    "        # nombramos al modelo con su grupo específico\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # Inicializamos el modelo y lo asignamos al diccionario\n",
    "        models[model_name_group] = LogisticRegression()\n",
    "\n",
    "        # Realizamos el modelo usando datos para cada grupo en el validation data\n",
    "        models[ model_name_group ] = models[model_name_group].fit(data_group_list[0].scores,   # data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\n",
    "                                                                  data_val_copy.subset(data_group_list[1]).labels.ravel()) # data_group_list[1] -> priv_indices or unpriv_indices\n",
    "        # haz predicciones de probabilidad con el modelo de cada groupo y guardar lo en data_val_preds2\n",
    "        # las predicciones de probabilidad son los platt scores\n",
    "        scores_group = models[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(models[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        data_val_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "        data_val_preds2_m.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "        \n",
    "        # Evaluamos a los modelos para el rango de umbrales\n",
    "    thresh_sweep_platt = np.linspace(np.min(data_val_preds2.scores.ravel()),\n",
    "                                     np.max(data_val_preds2.scores.ravel()),\n",
    "                                     50)\n",
    "\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep_from_scores_mult(\n",
    "            dataset_true = data_val_copy,\n",
    "            dataset2 = data_val2_copy,\n",
    "            dataset_preds = data_val_preds2_m,\n",
    "            thresh_arr = thresh_sweep_platt\n",
    "        )\n",
    "\n",
    "    # Evaluamos las métricas para el mejor umbral y las guardamos\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics( metrics_sweep[model_name+fairness_method] )\n",
    "\n",
    "    # Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "    print('\\nValidation set results for {}'.format(model_name+fairness_method))\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "    # utils.plot_fairness_and_accuracy( metrics_sweep[model_name+fairness_method] )\n",
    "    # plt.title( model_name+fairness_method )\n",
    "    \n",
    "    f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "    a.set_title(model_name+fairness_method )\n",
    "    f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244a8f4-6a63-4d85-a705-aa742b676989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# usa el optimal cutoff para predecir usando el test dato\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Obtenemos los scores del modelo de referencia en el teest data\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name], class_thresh = model_thresh)\n",
    "    data_test_preds_m = utils.update_dataset_from_model2(data_test_copy, data_test2_copy, models[model_name], class_thresh = model_thresh)\n",
    "    \n",
    "    # 1. dividir el test data usando los groups sensibles\n",
    "    data_test_preds_priv, data_test_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(dataset = data_test_preds,\n",
    "                                                                                                                            privileged_group_label = list((privileged_groups[0].values()))[0])\n",
    "    # 2. copia el test data para guardar los platt scores\n",
    "    data_test_preds2 = data_test_preds.copy(deepcopy = True)\n",
    "    data_test_preds2_m = data_test_preds_m.copy(deepcopy = True)\n",
    "    \n",
    "    # 3. predecir con el modelo de cada grupo\n",
    "    sensitive_groups_data_test = {'priv': [data_test_preds_priv, priv_indices],\n",
    "                                  'unpriv': [data_test_preds_unpriv, unpriv_indices]}\n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data_test.items():    \n",
    "        # nombramos al modelo con su grupo específico\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # haz predicciones de probabilidad con el modelo de cada groupo y guardar lo en data_test_preds2\n",
    "        scores_group = models[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(models[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        data_test_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "    \n",
    "    metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores_mult(dataset_true = data_test_copy,\n",
    "                                                                                             dataset_pred = data_test_preds2_m,\n",
    "                                                                                             dataset2 = data_test2_copy,\n",
    "                                                                                             threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'])\n",
    "    print('\\nTest set results for {}'.format(model_name+fairness_method))\n",
    "    utils.print_metrics(metrics_best_thresh_test[model_name+fairness_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0198e-151d-4b97-a075-a280858cc74e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Procesador de probabilidades igualadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f6934-8815-4a36-82c5-b274ebaf15be",
   "metadata": {},
   "source": [
    "#### Usando labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5adde4-4daa-4b1d-a0a5-be3cbd26108d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_equal_odds'\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "\n",
    "#data_train2_preds = utils.update_dataset_from_model2(data_train_copy, data_train2_copy, models[model_name])\n",
    "#data_val2_preds = utils.update_dataset_from_model2(data_val_copy, data_val2_copy, models[model_name])\n",
    "#data_test2_preds = utils.update_dataset_from_model2(data_test_copy, data_test2_copy, models[model_name])\n",
    "\n",
    "# Inicializamos el modelo (dadas las predicciones, no los scores) y lo asignamos al diccionario\n",
    "models[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    #cost_constraint = quality,\n",
    "    seed = seed)\n",
    "\n",
    "# Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd80d7-0433-43b3-b1b6-f109d2264f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "    dataset_true=data_val_copy,\n",
    "    dataset_preds=data_val_preds,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep,\n",
    "    scores_or_labels='labels'\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "    dataset_true=data_test_copy,\n",
    "    dataset2 = data_test2_copy,\n",
    "    dataset_preds=data_test_preds,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "    scores_or_labels='labels')\n",
    "\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "print('\\n')\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e19fa-8556-4a70-943b-ec9b47ceb3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_equal_odds'\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'xgb_classifier'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "#data_train2_preds = utils.update_dataset_from_model2(data_train_copy, data_train2_copy, models[model_name])\n",
    "#data_val2_preds = utils.update_dataset_from_model2(data_val_copy, data_val2_copy, models[model_name])\n",
    "#data_test2_preds = utils.update_dataset_from_model2(data_test_copy, data_test2_copy, models[model_name])\n",
    "\n",
    "\n",
    "# Inicializamos el modelo (dadas las predicciones, no los scores) y lo asignamos al diccionario\n",
    "models[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    cost_constraint = quality,\n",
    "    seed = seed)\n",
    "\n",
    "# Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "models[model_name+fairness_method] = models[model_name+fairness_method].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e34534-b37f-4781-ac6b-244f0bc2083d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Evaluamos al modelo para un rango de umbrales\n",
    "metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "    dataset_true=data_val_copy,\n",
    "    dataset_preds=data_val_preds,\n",
    "    model=models[model_name+fairness_method],\n",
    "    thresh_arr=thresh_sweep,\n",
    "    scores_or_labels='labels'\n",
    ")\n",
    "\n",
    "# Evaluamos las métricas para el mejor umbral\n",
    "metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "# Usando el mejor umbral, calculamos las métricas en el test set\n",
    "metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "    dataset_true=data_test_copy,\n",
    "    dataset_preds=data_test_preds,\n",
    "    dataset2=data_test2_copy,\n",
    "    model=models[model_name+fairness_method], \n",
    "    threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "    scores_or_labels='labels')\n",
    "\n",
    "utils.print_metrics(metrics_best_thresh_validate[model_name+fairness_method])\n",
    "print('\\n')\n",
    "\n",
    "# Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "# utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "# plt.title(model_name+fairness_method)\n",
    "\n",
    "f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name+fairness_method])\n",
    "a.set_title(model_name+fairness_method)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0f34b-3472-4f07-99ca-682f1d43eec4",
   "metadata": {},
   "source": [
    "#### Usando scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe772f-abfd-47b1-af84-23ca8a8e2f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_equal_odds'\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "#data_train2_preds = utils.update_dataset_from_model2(data_train_copy, data_train2_copy, models[model_name])\n",
    "#data_val2_preds = utils.update_dataset_from_model2(data_val_copy, data_val2_copy, models[model_name])\n",
    "#data_test2_preds = utils.update_dataset_from_model2(data_test_copy, data_test2_copy, models[model_name])\n",
    "\n",
    "# Parámetros del modelo\n",
    "# métricas fair a optimizar\n",
    "quality_constraints = [\"weighted\", 'fnr', 'fpr'] # \"weighted\" average of fnr and fpr\n",
    "\n",
    "for quality in quality_constraints:\n",
    "    # nombramos al modelo con su métrica específica\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Inicializamos el modelo y lo asignamos al diccionario\n",
    "    models[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "\n",
    "    # Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "    models[model_name_metric] = models[model_name_metric].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3bf60-5655-4dfb-bc84-81328eeb70f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Validation set results for logistic regression**\"))\n",
    "for key_metric in quality_constraints:\n",
    "    \n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    print(f'Results for {model_name_metric}')\n",
    "    \n",
    "    # Evaluamos al modelo para un rango de umbrales\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=data_val_copy,\n",
    "        dataset_preds=data_val_preds, \n",
    "        model=models[model_name_metric],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='scores'\n",
    "    )\n",
    "\n",
    "    # Evaluamos las métricas para el mejor umbral\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    # Usando el mejor umbral, calculamos las métricas en el test set\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_test_copy,\n",
    "        dataset_preds=data_test_preds,\n",
    "        dataset2=data_test2_copy,\n",
    "        model=models[model_name_metric], \n",
    "        threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "        scores_or_labels='scores')\n",
    "\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name_metric])\n",
    "    print('\\n')\n",
    "\n",
    "    # Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "    # utils.plot_fairness_and_accuracy(metrics_sweep[model_name_metric])\n",
    "    # plt.title(model_name_metric)\n",
    "    \n",
    "    f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name_metric])\n",
    "    a.set_title(model_name_metric)\n",
    "    f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56a420-ac57-4e28-9501-eec5628cceb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Nombramos al modelo\n",
    "fairness_method = '_equal_odds'\n",
    "# Nombre del modelo al que le aplicaremos post-processing\n",
    "model_name = 'xgb_classifier'\n",
    "\n",
    "# Hacemos una copia de los datasets\n",
    "data_train_copy, data_val_copy, data_test_copy = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "data_train2_copy, data_val2_copy, data_test2_copy = data_train2.copy(deepcopy=True), data_val2.copy(deepcopy=True), data_test2.copy(deepcopy=True)\n",
    "\n",
    "# Hacemos una copia de las predicciones del modelo de referencia\n",
    "data_train_preds = utils.update_dataset_from_model(data_train_copy, models[model_name])\n",
    "data_val_preds = utils.update_dataset_from_model(data_val_copy, models[model_name])\n",
    "data_test_preds = utils.update_dataset_from_model(data_test_copy, models[model_name])\n",
    "\n",
    "\n",
    "# Parámetros del modelo\n",
    "# métricas fair a optimizar\n",
    "quality_constraints = [\"weighted\", 'fnr', 'fpr'] # \"weighted\" average of fnr and fpr\n",
    "\n",
    "for quality in quality_constraints:\n",
    "    # nombramos al modelo con su métrica específica\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Inicializamos el modelo y lo asignamos al diccionario\n",
    "    models[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "\n",
    "    # Realizamos el post-procesado en función de las clases realies (data_copy) y las predicciones del modelo de referencia (data_preds)\n",
    "    models[model_name_metric] = models[model_name_metric].fit(data_train_copy, data_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb743307-5205-4d92-8e71-faa2093a4c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"**Validation set results for xgb classifier**\"))\n",
    "for key_metric in quality_constraints:\n",
    "    \n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "    \n",
    "    print(f'Results for {model_name_metric}')\n",
    "    \n",
    "    # Evaluamos al modelo para un rango de umbrales\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=data_val_copy,\n",
    "        dataset_preds=data_val_preds,\n",
    "#        dataset2=data_val2_copy,\n",
    "        model=models[model_name_metric],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='scores'\n",
    "    )\n",
    "\n",
    "    # Evaluamos las métricas para el mejor umbral\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    # Usando el mejor umbral, calculamos las métricas en el test set\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "        dataset_true=data_test_copy,\n",
    "        dataset_preds=data_test_preds,\n",
    "        dataset2=data_test2_copy,\n",
    "        model=models[model_name_metric], \n",
    "        threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "        scores_or_labels='scores')\n",
    "\n",
    "    utils.print_metrics(metrics_best_thresh_validate[model_name_metric])\n",
    "    print('\\n')\n",
    "\n",
    "    # Graficamos el comportamiento del modelo para el rango de umbrales \n",
    "    # utils.plot_fairness_and_accuracy(metrics_sweep[model_name_metric])\n",
    "    # plt.title(model_name_metric)\n",
    "    \n",
    "    f,a = utils.plot_fairness_and_accuracy(metrics_sweep[model_name_metric])\n",
    "    a.set_title(model_name_metric)\n",
    "    f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ba458-01d8-4c53-b028-32d231932f50",
   "metadata": {},
   "source": [
    "## Comparación de modelos\n",
    "\n",
    "Podemos usar los diccionarios que resumen la calidad de los modelos fair para compararlos todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbebe87-e74a-48cf-b621-5781d0f187e3",
   "metadata": {},
   "source": [
    "### Resultados en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fa550-f68f-48a9-bec4-b06db42d0327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_validate).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06e761-8490-4df7-84ac-151da29f852a",
   "metadata": {},
   "source": [
    "### Resultados en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abfc6b-bd82-4291-8473-a1c5ccdd9bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b92c06-f74f-46f1-9768-d91a43e96850",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_performance_summary.to_csv('SIMULATION_MULTSENS_mild', index = True, sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (aif360)",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
